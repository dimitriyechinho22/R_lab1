---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

### *Andrii Vandzhura, Dimitrii Yevchenko, Kateryna Akhynko*

## Introduction

During the past three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the *Bayes
theorem*.

One of its applications is **Naive Bayes classifier**, which is a
probabilistic classifier whose aim is to determine which class some
observation probably belongs to by using the Bayes formula:
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}
(\mathrm{observation})}$$

Under the strong independence assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given
observation. Thus, $\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now
can be calculated as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated from the data as
respective relative frequencies;\
see [this
site](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/)
for more detailed explanations.

## Data description

There are 5 data sets uploaded on the cms.

To determine your variant, take your team number from the list of teams
on cms and take *mod 5* - this is the number of your data set.

-   **0 - authors** This data set consists of citations of three famous
    writers: Edgar Alan Poe, Mary Wollstonecraft Shelley and HP
    Lovecraft. The task with this data set is to classify a piece of
    text with the author who was more likely to write it.

-   **1 - discrimination** This data set consists of tweets that have
    discriminatory (sexism or racism) messages or of tweets that are of
    neutral mood. The task is to determine whether a given tweet has
    discriminatory mood or does not.

-   **2 - fake news** This data set contains data of American news: a
    headline and an abstract of the article. Each piece of news is
    classified as fake or credible. The task is to classify the news
    from test.csv as credible or fake.

-   **3 - sentiment** All the text messages contained in this data set
    are labeled with three sentiments: positive, neutral or negative.
    The task is to classify some text message as the one of positive
    mood, negative or neutral.

-   **4 - spam** This last data set contains SMS messages classified as
    spam or non-spam (ham in the data set). The task is to determine
    whether a given message is spam or non-spam.

Each data set consists of two files: *train.csv* and *test.csv*. The
first one you will need find the probabilities distributions for each of
the features, while the second one is needed for checking how well your
classifier works. \### (Addition) We added additional library called
MLmetrics in order to perform additional accuracy scoring And later we
compare values of our own accuracy estimation to the F1 metric

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
library(MLmetrics)
```

## Instructions

-   The first step is data pre-processing, which includes removing
    punctuation marks and stop words

-   represent each message as a bag-of-words

-   using the training set, calculate all the conditional probabilities
    in formula (1)

-   use those to predict classes for messages in the test set

-   evaluate effectiveness of the classifier by calculating the
    corresponding metrics

-   shortly summarize your work

-   do not forget to submit both the (compiled) Rmd source file and the
    .html output

### Data pre-processing

-   Read the *.csv* data files.
-   Ð¡lear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.
-   It is highly recommended to get familiar with R data frames, it
    would make the work much easier to do.
-   Useful links:
    -   <https://steviep42.github.io/webscraping/book/bagofwords.html#tidytext> -
        example of using *tidytext* to count frequencies of the words.
    -   Basics of Text Mining in R:
        <http://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html>
        . Note that it also includes an example on how to create a bag
        of words from your text document.

### Reading csv

```{r}
train_path <- "train.csv"
test_path <- "test.csv"
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test <- read.csv(file = test_path, stringsAsFactors = FALSE)
```

# Removing punctuation and stop words

```{r}
stop_words_path <- "stop_words.txt"
stop_words <- read_file(stop_words_path)
splitted_stop_words <- strsplit(stop_words, split = '\n')
splitted_stop_words <- splitted_stop_words[[1]]
```

### Implementing and testing classifier

## Classifier implementation

The first we decided to implement classifier class, which is responsible
for analising our data and predicting the credibility of certain piece
of news

Implemented class fields: - data: all data - labels: array with fake and
credible - fake_article_probability: probability of fake -
credible_article_probability: probability of credible - all_vocabulary:
data frame with all occurrences - fake_words_dictionary: data frame with
fake words occurrences - credible_words_dictionary: data frame with
credible words occurrences

Implemented class functions: - fit(X = data, Y = labels): creates all
the useful data frames - probability_of_one_word(wordi, lable):
calculates the probability of certain word to be credible or fake news -
predict(message): predicts the credibility of certain news to be
credible or fake based on Bayes' formula - score(X_test): creates
convenient data frame with predictions used for scoring effectiveness of
model

```{r}
naiveBayes <- setRefClass("naiveBayes",
                          # here it would be wise to have some vars to store intermediate result
                          # frequency dict etc. Though pay attention to bag of wards!
                          fields = list(
                            data = "data.frame",                       # all data
                            labels = "vector",                         # array with fake and credible
                            fake_article_probability = "numeric",      # probability of fake
                            credible_article_probability = "numeric",  # probability of credible
                            all_vocabulary = "data.frame",             # dataframe with all occurences
                            fake_words_dictionary = "data.frame",      # dataframe with fake words occurences
                            credible_words_dictionary = "data.frame"   # dataframe with credible words occurences
                          ),

                          methods = list(
                            fit = function(X = data, Y = labels) {
                              tidy_text <- unnest_tokens(X, 'splitted', 'Body', token = "words") %>%
                                filter(!splitted %in% splitted_stop_words)

                              fake_article_probability <<- nrow(filter(train, Label == Y[2])) / nrow(X)
                              credible_article_probability <<- 1 - fake_article_probability

                              # all vocabulary
                              all_vocabulary <<- tidy_text %>% count(splitted, sort = TRUE)

                              # fake vocabulary
                              fake_words_dictionary <<- tidy_text[tidy_text$Label == Y[2],] %>% count(splitted, sort = TRUE) # creates {key : value}

                              # credible vocabulary
                              credible_words_dictionary <<- tidy_text[tidy_text$Label == Y[1],] %>% count(splitted, sort = TRUE)
                            },

                            probability_of_one_word = function(wordi, lable) {
                              lable_dictionary <- fake_words_dictionary
                              if (lable == "credible") {
                                lable_dictionary <- credible_words_dictionary
                              }
                              occurence <- lable_dictionary[lable_dictionary$splitted == wordi, "n"]  # amount of words
                              if (length(occurence) == 0) {
                                return(1 / (nrow(all_vocabulary) + nrow(lable_dictionary)))
                              }
                              else {
                                return((occurence + 1) / (nrow(all_vocabulary) + nrow(lable_dictionary)))
                              }
                            },

                            predict = function(message) {
                              result_for_fake <- fake_article_probability
                              result_for_credible <- credible_article_probability

                              multiplier <- 10000

                              message <- gsub('[[:punct:] ]+', ' ', message)
                              message <- casefold(message, upper = FALSE)
                              words <- strsplit(message, split = ' ')
                              words <- words[[1]]
                              words <- unlist(words)[!(unlist(words) %in% splitted_stop_words)]
                              for (word in words) {
                                result_for_fake <- result_for_fake *
                                  probability_of_one_word(word, lables[2]) * multiplier
                                result_for_credible <- result_for_credible *
                                  probability_of_one_word(word, lables[1]) * multiplier
                              }

                              if (result_for_fake > result_for_credible) {
                                return ("fake")
                              } else {
                                return ("credible")
                              }
                            },

                            score = function (X_test) {
                              X_test["prediction"] <- apply(X_test['Body'],1, FUN = model$predict)
                              return (X_test)
                            }

                          )
)
```

## Accuracy estimation

Estimates accuracy of the model based on the given DF. In model_accuracy
function we calculate it by comparing values of two columns on each row.
(\~81,8%) In F1_scoring function we calculate the accuracy using F1
scoring method (importing library for that (\~85%)

```{r}
model_accuracy <- function(dataframe) {
  correct_predictions <- 0
  articles_amount <- nrow(dataframe)

  for (i in 1:articles_amount) {
    actual_value <- dataframe$Label[i]
    predicted_value <- dataframe$prediction[i]
    if (actual_value == predicted_value) {
      correct_predictions <- correct_predictions + 1
    }
  }

  accuracy_value <- correct_predictions / articles_amount
  return(accuracy_value)
}

F1_scoring <- function (dataframe){
  actual <- dataframe$Label
  predicted <- dataframe$prediction

  print(F1_Score(predicted,actual))
}
```

## Estimating accuracy of classifier

We create Object of Class Naive Bayes, also creating some additional
data. We ran method Score to get info about credibility for each
article.

```{r}
lables <- unique(train$Label)
model <- naiveBayes(data = train, labels = unique(train$Label))
model$fit()
result <- model$score(test)

print("Model accuracy")
print(model_accuracy(result))  # ~81,8%
print(F1_scoring(result))      # ~85%
```

### Getting errors of classifier prediction

## Fake news classified as credible

In this section we create data frame with all the articles, where our
implementation of classifier Failed. (fake - credible)

```{r}
error_in_fake_analysis <- function(dataframe) {
  error_in_credible <- dataframe[dataframe$Label == "fake",]
  error_in_credible2 <- error_in_credible[error_in_credible$prediction == "credible",]

  return (error_in_credible2)
}
error_in_fake <- error_in_fake_analysis(result)
```

## Credible news classified as fake

In this section we create data frame with all the articles, where our
implementation of classifier Failed. (fake - credible)

```{r}
error_in_credible_analysis <- function(dataframe) {
  error_in_credible <- dataframe[dataframe$Label == "credible",]
  error_in_credible2 <- error_in_credible[error_in_credible$prediction == "fake",]

  return (error_in_credible2)
}
error_in_credible <- error_in_credible_analysis(result)  # dataframe with (fake - credible)

```

### Data visualization

## Diagrams realization

Function create_diagrams is used to create diagrams (DATA Visualization)
Arguments: num_fake: amount of fake news num_credible: amount of
credible name_fake: name for piece of fake news name_credible: name for
piece of credible news title: title of chart

```{r}
create_diagrams <- function(num_fake, num_credible, name_fake, name_credible, title) {
  # pie chart of true numbers of fake and credible articles

  first_column <- c(name_credible, name_fake)
  second_column <- c(num_credible, num_fake)
  df <- data.frame(first_column, second_column)

  df <- df %>%
    arrange(desc(first_column)) %>%
    mutate(prop = second_column / sum(df$second_column) * 100) %>%
    mutate(ypos = cumsum(prop)- 0.5*prop )

  pie_chart <-ggplot(df, aes(x="", y=prop, fill=first_column)) +
    geom_bar(stat="identity", width=3, color="white") +
    coord_polar("y", start=0) +
    theme_void() +
    theme(legend.title=element_blank(), legend.position="right", legend.text = element_text(size=12))+
    geom_text(aes(y = ypos, label = as.character(second_column)), color = "white", size=5) +
    scale_fill_brewer(palette="Set5") +
    ggtitle(title, ) +
    scale_fill_hue(l=45)
  pie_chart
}
```

## Diagrams creation

This chunk creates 2 diagrams. First diagram represents distribution
between fake and credible news as it is. This data was given in file. We
create second diagram with distribution between fake and credible news,
but data taken from Naive Bayes calculations These two diagrams helps us
visually analyze our algorithm. As you will see, there is no big
difference

```{r}
num_fake_articles_test <- nrow(filter(test, Label == "fake"))
num_credible_articles_test <- nrow(filter(test, Label == "credible"))
names_initial_credible <- "number of credible articles in test.csv"
names_initial_fake <- "number of fake articles in test.csv"
initial_title <- "Initial number of fake and credible news in test.csv"
resulted_title <- "Number of fake and credible news according to Bayes Classifier"

fakes_amount <- nrow(result[result$prediction == "fake",])
credible_amount <- nrow(result[result$prediction == "credible",])

create_diagrams(num_fake_articles_test, num_credible_articles_test, names_initial_fake, names_initial_credible, initial_title)
create_diagrams(fakes_amount, credible_amount, "Predicted as fake", "Predicted as credible", resulted_title)
```

### Data visualization

Each time you work with some data, you need to understand it before you
start processing it. R has very powerful tools to make nice plots and
visualization. Show what are the most common words for negative and
positive examples as a histogram, word cloud etc. Be creative!

## Classifier implementation

## Measure effectiveness of your classifier

-   Note that accuracy is not always a good metric for your classifier.
    Look at precision and recall curves, F1 score metric.
-   Visualize them.
-   Show failure cases.

## Conclusion

-   Describe the method implemented in general. Show what are
    mathematical foundations you are basing your solution on.
-   List pros and cons of the method. This should include the
    limitations of your method, all the assumption you make about the
    nature of your data etc.

The Bayes classifier is based on the Bayes formula (a fundamental
formula of probability theory). Considering each word as independent, it
is necessary to multiply them for each specific message.

Bayes' algorithm does not treat a sentence as a collection that has
meaning.

P(word/Fake) = P(Fake/word) \* P(Fake)/P(word)

P(Fake/word) = y + 1/ x + vocab , where y - occurrences of our word in
fake set, y - set of all words in fake news, vocab - set of all words in
our data set.

We have P(word) but we don't count it in our program as for credible it
would be the same denominator. To count probability for article we
multiply all these conditional probabilities (as we consider them
independent) Bayes works with each word separately, treating it as an
independent part of the set. This algorithm is accurate, but has some
drawbacks. The science of natural language processing can provide many
examples of the incorrectness of Bayes' theorem. If we consider each
word as independent, we avoid the connection between cognate words, the
position of the word in the sentence, homonyms, synonyms, etc. We can
also see that F1 score metric works better than the one introduced by us
in the task Generally, our team was very excited doing this project. But
the main problem for us was the usage of R language and environments
connected to it. We actually knew the solution for this problem, but
inconvenient syntax of R and of absence of structured guide for it, made
the work 10 times longer. \<3
